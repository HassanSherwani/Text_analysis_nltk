{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis nltk\n",
    "\n",
    "Based on Rocky DeRaze youtube channel. Some updates are made as per syntax. There are some additional examples being used to clarify nlp concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}\n",
       "h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)- Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parag = \"WASHINGTON — In his first full cabinet meeting last June, President Trump invited a chorus of gushing praise from his top aides by boasting that he had assembled a “phenomenal team of people, a great group of talent.” \\\n",
    "But in the nine months since then, Mr. Trump has fired or forced out a half-dozen of the “incredible, talented” people in the Cabinet Room that day: his secretaries of state and health, along with his chief strategist, his chief of staff, his top economic aide and his press secretary. \\\n",
    "And the purge at the top may not be over. Mr. Trump, who is famously fickle, appears to have soured on additional members of his senior leadership team — and his frequent mulling about making changes has some people around him convinced that he could act soon. \\\n",
    "“There will always be change. I think you want to see change,” Mr. Trump said, ominously, on Thursday. “I want to also see different ideas.”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_parag = sent_tokenize(parag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WASHINGTON — In his first full cabinet meeting last June, President Trump invited a chorus of gushing praise from his top aides by boasting that he had assembled a “phenomenal team of people, a great group of talent.” But in the nine months since then, Mr. Trump has fired or forced out a half-dozen of the “incredible, talented” people in the Cabinet Room that day: his secretaries of state and health, along with his chief strategist, his chief of staff, his top economic aide and his press secretary.', 'And the purge at the top may not be over.', 'Mr. Trump, who is famously fickle, appears to have soured on additional members of his senior leadership team — and his frequent mulling about making changes has some people around him convinced that he could act soon.', '“There will always be change.', 'I think you want to see change,” Mr. Trump said, ominously, on Thursday.', '“I want to also see different ideas.”']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_parag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_parag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON — In his first full cabinet meeting last June, President Trump invited a chorus of gushing praise from his top aides by boasting that he had assembled a “phenomenal team of people, a great group of talent.” But in the nine months since then, Mr. Trump has fired or forced out a half-dozen of the “incredible, talented” people in the Cabinet Room that day: his secretaries of state and health, along with his chief strategist, his chief of staff, his top economic aide and his press secretary.\n",
      "\n",
      "And the purge at the top may not be over.\n",
      "\n",
      "Mr. Trump, who is famously fickle, appears to have soured on additional members of his senior leadership team — and his frequent mulling about making changes has some people around him convinced that he could act soon.\n",
      "\n",
      "“There will always be change.\n",
      "\n",
      "I think you want to see change,” Mr. Trump said, ominously, on Thursday.\n",
      "\n",
      "“I want to also see different ideas.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in tokenizer.tokenize(parag):\n",
    "\tprint(w+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayT = tokenizer.tokenize(parag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I want to also see different ideas.”\n"
     ]
    }
   ],
   "source": [
    "print(arrayT[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the purge at the top may not be over.\n"
     ]
    }
   ],
   "source": [
    "print(arrayT[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)- Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Let\\'s hack this freaking Linux droid! We shan't wait.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "\n",
    "tok3 = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'hack', 'this', 'freaking', 'Linux', 'droid', '!', 'We', 'sha', \"n't\", 'wait', '.']\n",
      "['Let', \"'s\", 'hack', 'this', 'freaking', 'Linux', 'droid', '!', 'We', 'sha', \"n't\", 'wait', '.']\n",
      "['Let', \"'\", 's', 'hack', 'this', 'freaking', 'Linux', 'droid', '!', 'We', 'shan', \"'\", 't', 'wait', '.']\n"
     ]
    }
   ],
   "source": [
    "print(arr)\n",
    "print(tok2.tokenize(sent1))\n",
    "print(tok3.tokenize(sent1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference how WordPunctTokenizer works with punctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)-Regular Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parag1 = \"I won't do this, you shan't do that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wo', \"n't\", 'do', 'this', ',', 'you', 'sha', \"n't\", 'do', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(parag1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"won't\", 'do', 'this', 'you', \"shan't\", 'do', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(parag1, \"[\\w']+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem for word segmentation is pretty much solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'w', 'o', 'n', \"'\", 't', 'd', 'o', 't', 'h', 'i', 's', 'y', 'o', 'u', 's', 'h', 'a', 'n', \"'\", 't', 'd', 'o', 't', 'h', 'a', 't']\n"
     ]
    }
   ],
   "source": [
    "# without + at end\n",
    "print(regexp_tokenize(parag1, \"[\\w']\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'won', 't', 'do', 'this', 'you', 'shan', 't', 'do', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(parag1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)-Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(ensw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ensw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parag1 = \"WASHINGTON — In his first full cabinet meeting last June, President Trump invited a chorus of gushing praise from his top aides by boasting that he had assembled a “phenomenal team of people, a great group of talent.” \\\n",
    "But in the nine months since then, Mr. Trump has fired or forced out a half-dozen of the “incredible, talented” people in the Cabinet Room that day: his secretaries of state and health, along with his chief strategist, his chief of staff, his top economic aide and his press secretary. \\\n",
    "And the purge at the top may not be over. Mr. Trump, who is famously fickle, appears to have soured on additional members of his senior leadership team — and his frequent mulling about making changes has some people around him convinced that he could act soon. \\\n",
    "“There will always be change. I think you want to see change,” Mr. Trump said, ominously, on Thursday. “I want to also see different ideas.”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragArr = word_tokenize(parag1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', '—', 'in', 'his', 'first', 'full', 'cabinet', 'meeting', 'last', 'june', ',', 'president', 'trump', 'invited', 'a', 'chorus', 'of', 'gushing', 'praise', 'from', 'his', 'top', 'aides', 'by', 'boasting', 'that', 'he', 'had', 'assembled', 'a', '“', 'phenomenal', 'team', 'of', 'people', ',', 'a', 'great', 'group', 'of', 'talent.', '”', 'but', 'in', 'the', 'nine', 'months', 'since', 'then', ',', 'mr.', 'trump', 'has', 'fired', 'or', 'forced', 'out', 'a', 'half-dozen', 'of', 'the', '“', 'incredible', ',', 'talented', '”', 'people', 'in', 'the', 'cabinet', 'room', 'that', 'day', ':', 'his', 'secretaries', 'of', 'state', 'and', 'health', ',', 'along', 'with', 'his', 'chief', 'strategist', ',', 'his', 'chief', 'of', 'staff', ',', 'his', 'top', 'economic', 'aide', 'and', 'his', 'press', 'secretary', '.', 'and', 'the', 'purge', 'at', 'the', 'top', 'may', 'not', 'be', 'over', '.', 'mr.', 'trump', ',', 'who', 'is', 'famously', 'fickle', ',', 'appears', 'to', 'have', 'soured', 'on', 'additional', 'members', 'of', 'his', 'senior', 'leadership', 'team', '—', 'and', 'his', 'frequent', 'mulling', 'about', 'making', 'changes', 'has', 'some', 'people', 'around', 'him', 'convinced', 'that', 'he', 'could', 'act', 'soon', '.', '“', 'there', 'will', 'always', 'be', 'change', '.', 'i', 'think', 'you', 'want', 'to', 'see', 'change', ',', '”', 'mr.', 'trump', 'said', ',', 'ominously', ',', 'on', 'thursday', '.', '“', 'i', 'want', 'to', 'also', 'see', 'different', 'ideas', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "print(paragArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprehension\n",
    "filterArr = [item for item in paragArr if item not in ensw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', '—', 'first', 'full', 'cabinet', 'meeting', 'last', 'june', ',', 'president', 'trump', 'invited', 'chorus', 'gushing', 'praise', 'top', 'aides', 'boasting', 'assembled', '“', 'phenomenal', 'team', 'people', ',', 'great', 'group', 'talent.', '”', 'nine', 'months', 'since', ',', 'mr.', 'trump', 'fired', 'forced', 'half-dozen', '“', 'incredible', ',', 'talented', '”', 'people', 'cabinet', 'room', 'day', ':', 'secretaries', 'state', 'health', ',', 'along', 'chief', 'strategist', ',', 'chief', 'staff', ',', 'top', 'economic', 'aide', 'press', 'secretary', '.', 'purge', 'top', 'may', '.', 'mr.', 'trump', ',', 'famously', 'fickle', ',', 'appears', 'soured', 'additional', 'members', 'senior', 'leadership', 'team', '—', 'frequent', 'mulling', 'making', 'changes', 'people', 'around', 'convinced', 'could', 'act', 'soon', '.', '“', 'always', 'change', '.', 'think', 'want', 'see', 'change', ',', '”', 'mr.', 'trump', 'said', ',', 'ominously', ',', 'thursday', '.', '“', 'want', 'also', 'see', 'different', 'ideas', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "print(filterArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filterArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)-Synsets, Hypernyms and Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"Movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "synArray = wordnet.synsets(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('movie.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(synArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word of interest\n",
    "woi = synArray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('movie.n.01')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie.n.01'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('product.n.02'), Synset('show.n.03')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cinema_verite.n.01'),\n",
       " Synset('collage_film.n.01'),\n",
       " Synset('coming_attraction.n.01'),\n",
       " Synset('documentary.n.01'),\n",
       " Synset('feature.n.03'),\n",
       " Synset('film_noir.n.01'),\n",
       " Synset('final_cut.n.01'),\n",
       " Synset('home_movie.n.01'),\n",
       " Synset('musical.n.01'),\n",
       " Synset('rough_cut.n.01'),\n",
       " Synset('shoot-'em-up.n.01'),\n",
       " Synset('short_subject.n.01'),\n",
       " Synset('silent_movie.n.01'),\n",
       " Synset('skin_flick.n.01'),\n",
       " Synset('slow_motion.n.01'),\n",
       " Synset('talking_picture.n.01'),\n",
       " Synset('telefilm.n.01'),\n",
       " Synset('three-d.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "woi2 = woi.hyponyms()[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('silent_movie.n.01')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('movie.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi2.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a movie without a soundtrack'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi2.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)-Lemmas, Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sArr = wordnet.synsets('win')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('win.n.01'),\n",
       " Synset('winnings.n.01'),\n",
       " Synset('win.v.01'),\n",
       " Synset('acquire.v.05'),\n",
       " Synset('gain.v.05'),\n",
       " Synset('succeed.v.01')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "woi = sArr[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('win.v.01.win')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be the winner in a contest or competition; be victorious'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'win'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woi.lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "synArr = []\n",
    "antArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in sArr:\n",
    "\tfor lem in syn.lemmas():\n",
    "\t\tsynArr.append(lem.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['win', 'winnings', 'win', 'profits', 'win', 'acquire', 'win', 'gain', 'gain', 'advance', 'win', 'pull_ahead', 'make_headway', 'get_ahead', 'gain_ground', 'succeed', 'win', 'come_through', 'bring_home_the_bacon', 'deliver_the_goods']\n"
     ]
    }
   ],
   "source": [
    "print(synArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(synArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'winnings', 'make_headway', 'get_ahead', 'come_through', 'pull_ahead', 'succeed', 'win', 'gain', 'advance', 'acquire', 'profits', 'deliver_the_goods', 'bring_home_the_bacon', 'gain_ground'}\n"
     ]
    }
   ],
   "source": [
    "# to get a unique value, we will use \"set\"\n",
    "print(set(synArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(set(synArr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('lose.v.02.lose')]\n"
     ]
    }
   ],
   "source": [
    "print(woi.lemmas()[0].antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in sArr:\n",
    "\tfor lem in syn.lemmas():\n",
    "\t\tfor ant in lem.antonyms():\n",
    "\t\t\tantArr.append(ant.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['losings', 'lose', 'lose', 'fall_back', 'fail']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(antArr)\n",
    "print(len(antArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fall_back', 'lose', 'fail', 'losings'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(set(antArr))\n",
    "print(len(set(antArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)-Wu Palmer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarr1 = wordnet.synsets('cake')\n",
    "sarr2 = wordnet.synsets('loaf')\n",
    "sarr3 = wordnet.synsets('bread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cake.n.01'), Synset('patty.n.01'), Synset('cake.n.03'), Synset('coat.v.03')]\n"
     ]
    }
   ],
   "source": [
    "print(sarr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('loaf_of_bread.n.01'), Synset('loaf.n.02'), Synset('bum.v.02'), Synset('loiter.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(sarr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bread.n.01'), Synset('boodle.n.01'), Synset('bread.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(sarr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cake = sarr1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "loafb = sarr2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaf = sarr2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = sarr3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3076923076923077\n"
     ]
    }
   ],
   "source": [
    "print(cake.wup_similarity(loaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning that cake and loaf are 30.76% similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666\n",
      "0.7142857142857143\n",
      "0.7692307692307693\n",
      "0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "print(cake.wup_similarity(loafb))\n",
    "print(loaf.wup_similarity(loafb))\n",
    "print(bread.wup_similarity(loaf))\n",
    "print(bread.wup_similarity(loafb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('food.n.02')\n"
     ]
    }
   ],
   "source": [
    "print(loaf.hypernyms()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = loaf.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(loaf.shortest_path_distance(ref))\n",
    "print(bread.shortest_path_distance(ref))\n",
    "print(loafb.shortest_path_distance(ref))\n",
    "print(cake.shortest_path_distance(ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)-Path and LCH Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "catArr = wordnet.synsets(\"cat\")\n",
    "dogArr = wordnet.synsets(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cat.n.01'),\n",
       " Synset('guy.n.01'),\n",
       " Synset('cat.n.03'),\n",
       " Synset('kat.n.01'),\n",
       " Synset('cat-o'-nine-tails.n.01'),\n",
       " Synset('caterpillar.n.02'),\n",
       " Synset('big_cat.n.01'),\n",
       " Synset('computerized_tomography.n.01'),\n",
       " Synset('cat.v.01'),\n",
       " Synset('vomit.v.01')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = dogArr[0]\n",
    "coi = catArr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cat.n.01')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi.wup_similarity(coi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi.path_similarity(coi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity of 1 or value closer to 1 describes sameness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi.path_similarity(doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leacock Chodorow similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0281482472922856"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi.lch_similarity(coi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6375861597263857"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi.lch_similarity(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0281482472922856"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coi.lch_similarity(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6375861597263857"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coi.lch_similarity(coi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this 3.63 is the highest value as it shows relation of dog with dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9)-Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext \n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Hassan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data\n",
    "textWords = [w.lower() for w in webtext.words('pirates.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22679"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(textWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeliestW = finder.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'\", 's'), ('jack', 'sparrow'), (']', '['), ('will', 'turner'), ('sparrow', ':'), ('elizabeth', 'swann'), ('turner', ':'), ('davy', 'jones'), ('swann', ':'), (\"'\", 't'), ('flying', 'dutchman'), ('lord', 'cutler'), ('cutler', 'beckett'), ('black', 'pearl'), ('gibbs', ':'), ('[', 'jack'), ('tia', 'dalma'), (\"'\", 're'), ('of', 'the'), ('!', '[')]\n"
     ]
    }
   ],
   "source": [
    "print(likeliestW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(likeliestW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "from nltk.corpus import stopwords \n",
    "ignored_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterStops = lambda w: len(w) < 3 or w in ignored_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_word_filter(filterStops)\n",
    "likeliestW = finder.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jack', 'sparrow'), ('elizabeth', 'swann'), ('davy', 'jones'), ('flying', 'dutchman'), ('lord', 'cutler'), ('cutler', 'beckett'), ('black', 'pearl'), ('tia', 'dalma'), ('cannibal', 'island'), ('port', 'royal'), ('bamboo', 'pole'), ('edinburgh', 'trader'), ('east', 'india'), ('india', 'trading'), ('wounded', 'sailor'), ('black', 'spot'), ('scuttled', 'ship'), ('isla', 'cruces'), ('slow', 'motion'), ('trading', 'company')]\n"
     ]
    }
   ],
   "source": [
    "print(likeliestW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names like Jack Sparrow and Davy Jones are examples of Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10)- Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.corpus import webtext \n",
    "from nltk.metrics import TrigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "textWords = [w.lower() for w in webtext.words('grail.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16967"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop', ']', 'king', 'arthur', ':', 'whoa', 'there', '!', '[', 'clop', 'clop', 'clop', ']', 'soldier', '#', '1', ':', 'halt', '!', 'who', 'goes', 'there', '?', 'arthur', ':', 'it', 'is', 'i', ',', 'arthur', ',', 'son', 'of', 'uther', 'pendragon', ',', 'from', 'the', 'castle', 'of', 'camelot']\n"
     ]
    }
   ],
   "source": [
    "print(textWords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = TrigramCollocationFinder.from_words(textWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeliestW = finder.nbest(TrigramAssocMeasures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'boom', ']'), ('[', 'singing', ']'), ('[', 'music', ']'), ('[', 'clang', ']'), ('.', 'arthur', ':'), ('[', 'chanting', ']'), ('[', 'pause', ']'), ('[', 'squeak', ']'), ('[', 'thud', ']'), ('[', 'bonk', ']'), ('[', 'clunk', ']'), ('[', 'crash', ']'), ('[', 'howl', ']'), ('[', 'roar', ']'), ('[', 'kick', ']'), ('[', 'trumpets', ']'), ('[', 'twang', ']'), ('[', 'twong', ']'), ('[', 'whop', ']'), ('[', 'clank', ']')]\n"
     ]
    }
   ],
   "source": [
    "print(likeliestW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(likeliestW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again applying stopwords and filters\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "ignored_words = set(stopwords.words('english'))\n",
    "\n",
    "filterStops = lambda w: len(w) < 3 or w in ignored_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_word_filter(filterStops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeliestW = finder.nbest(TrigramAssocMeasures.likelihood_ratio, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clop', 'clop', 'clop'), ('mumble', 'mumble', 'mumble'), ('squeak', 'squeak', 'squeak'), ('saw', 'saw', 'saw'), ('black', 'knight', 'kills'), ('black', 'knight', 'always'), ('pie', 'iesu', 'domine'), ('clap', 'clap', 'clap'), ('squeak', 'squeak', '...]'), ('...', 'head', 'knight'), ('dona', 'eis', 'requiem'), ('brave', 'sir', 'robin'), ('holy', 'grail', 'returns'), ('holy', 'grail', 'could'), ('heh', 'heh', 'heh'), ('king', 'arthur', 'music'), ('iesu', 'domine', '...'), ('haw', 'haw', 'haw'), ('hee', 'hee', 'hee'), ('bold', 'sir', 'robin'), ('round', 'table', 'narrator'), ('sir', 'robin', 'ran'), ('sir', 'robin', 'rode'), ('sir', 'robin', 'set'), ('round', 'table', 'shall'), ('sir', 'robin', 'turned'), ('round', 'table', 'scene'), ('holy', 'hand', 'grenade'), ('arthur', 'music', 'stops'), ('boom', 'boom', 'boom')]\n"
     ]
    }
   ],
   "source": [
    "print(likeliestW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeliestW = finder.nbest(TrigramAssocMeasures.likelihood_ratio, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clop', 'clop', 'clop'), ('mumble', 'mumble', 'mumble'), ('squeak', 'squeak', 'squeak'), ('saw', 'saw', 'saw'), ('pie', 'iesu', 'domine'), ('clap', 'clap', 'clap'), ('dona', 'eis', 'requiem'), ('brave', 'sir', 'robin'), ('heh', 'heh', 'heh'), ('king', 'arthur', 'music'), ('hee', 'hee', 'hee'), ('holy', 'hand', 'grenade'), ('boom', 'boom', 'boom'), ('...', 'dona', 'eis'), ('already', 'got', 'one'), ('good', 'sir', 'knight')]\n"
     ]
    }
   ],
   "source": [
    "print(likeliestW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11)- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying PorterSTemmer - least strict on word cutting\n",
    "pstemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danc\n",
      "dancer\n",
      "cook\n",
      "cookeri\n"
     ]
    }
   ],
   "source": [
    "print(pstemmer.stem('dancing'))\n",
    "print(pstemmer.stem('dancer'))\n",
    "print(pstemmer.stem('cooking'))\n",
    "print(pstemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying LancasterStemmer - most strict on word cutting\n",
    "lstemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dant\n",
      "dant\n",
      "dant\n",
      "cook\n",
      "cookery\n"
     ]
    }
   ],
   "source": [
    "print(lstemmer.stem('dancing'))\n",
    "print(lstemmer.stem('dance'))\n",
    "print(lstemmer.stem('dancer'))\n",
    "print(lstemmer.stem('cooking'))\n",
    "print(lstemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexStemmer\n",
    "rstemmer = RegexpStemmer('ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ski\n",
      "cook\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "print(rstemmer.stem('skiing'))\n",
    "print(rstemmer.stem('cooking'))\n",
    "print(rstemmer.stem('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ing in skiing and cooking is extra however; king's ing makes it means. Hence removing ing from king means only k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12)- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dancing\n",
      "working\n"
     ]
    }
   ],
   "source": [
    "print(lzr.lemmatize('dancing'))\n",
    "print(lzr.lemmatize('working'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**adding pos to lemmatizer make it more clear if word is a verb, noun or other form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dance\n",
      "work\n",
      "working\n"
     ]
    }
   ],
   "source": [
    "print(lzr.lemmatize('dancing', pos='v'))\n",
    "print(lzr.lemmatize('working', pos='v'))\n",
    "print(lzr.lemmatize('working', pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "print(lzr.lemmatize('kings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sings\n",
      "sing\n"
     ]
    }
   ],
   "source": [
    "print(lzr.lemmatize('sings'))\n",
    "print(lzr.lemmatize('sings', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abruptly\n"
     ]
    }
   ],
   "source": [
    "print(lzr.lemmatize('abruptly', pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seeing difference in lemma and stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stm = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danc\n",
      "dance\n"
     ]
    }
   ],
   "source": [
    "print(stm.stem('dancing'))\n",
    "print(lzr.lemmatize('dancing', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believ\n",
      "belief\n",
      "believe\n"
     ]
    }
   ],
   "source": [
    "print(stm.stem('believes'))\n",
    "print(lzr.lemmatize('believes'))\n",
    "print(lzr.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buse\n",
      "bus\n",
      "bu\n"
     ]
    }
   ],
   "source": [
    "print(stm.stem('buses'))\n",
    "print(lzr.lemmatize('buses'))\n",
    "print(stm.stem('bus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13)-Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'(?i)don\\'t')\n",
    "fst = \"Don't you dare. I don't.\"\n",
    "sst = regex.sub('do not', fst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't you dare. I don't.\n",
      "do not you dare. I do not.\n"
     ]
    }
   ],
   "source": [
    "print(fst)\n",
    "print(sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst = \"I won't go there. He's a mad man. He won't end that. He'd have to go now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "givenpatterns = [\n",
    "\t\t\t\t\t(r'won\\'t', 'will not'), \n",
    "\t\t\t\t\t(r'\\'s', ' is'), \n",
    "\t\t\t\t\t(r'\\'d', ' would'),\n",
    "\t\t\t\t\t(r'mad man', 'crazy arse mother fucking anthropoid')\n",
    "\t\t\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(text, patterns):\n",
    "\tfor(raw, rep) in patterns:\n",
    "\t\tregex = re.compile(raw)\n",
    "\t\ttext = regex.sub(rep,text)\n",
    "\tprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I won't go there. He's a mad man. He won't end that. He'd have to go now.\n",
      "I will not go there. He is a crazy arse mother fucking anthropoid. He will not end that. He would have to go now.\n"
     ]
    }
   ],
   "source": [
    "print(fst)\n",
    "replace(fst, givenpatterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "regex = re.compile(r'(\\w*)(\\w)\\2(\\w*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = 'dramaaaatiiiic'\n",
    "sw = regex.sub(r'\\1\\2\\3', fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dramaaaatiiic\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looper(word):\n",
    "\tloop_res = regex.sub(r'\\1\\2\\3', word)\n",
    "\tif (word == loop_res):\n",
    "\t\treturn loop_res\n",
    "\telse:\n",
    "\t\t# Too see the process, uncomment the line:\n",
    "\t\t# print(loop_res)\n",
    "\t\treturn looper(loop_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = looper(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dramatic'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14)-Replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replacer import RegexReplacer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "givenpatterns = [\n",
    "\t\t\t\t\t(r'won\\'t', 'will not'), \n",
    "\t\t\t\t\t(r'\\'s', ' is'), \n",
    "\t\t\t\t\t(r'\\'d', ' would'),\n",
    "\t\t\t\t\t(r'mad man', 'crazy arse mother fucking anthropoid')\n",
    "\t\t\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer = RegexReplacer(givenpatterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = replacer.replace(\"He's gone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is gone'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RepeatReplacer\n",
    "from replacer import RepeatReplacer\n",
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = replacer.replace(\"Anthhhhropoiiid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anthropoid'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book\n"
     ]
    }
   ],
   "source": [
    "txt = replacer.replace(\"Book\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cattle\n"
     ]
    }
   ],
   "source": [
    "txt = replacer.replace(\"cattle\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle\n"
     ]
    }
   ],
   "source": [
    "txt = replacer.replace(\"botttleeee\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace same meaning word i.e Synonym\n",
    "from replacer import WordReplacer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmapobj = {\n",
    "\t\t\t\t'bday' : 'birthday',\n",
    "\t\t\t\t'sup' : 'what\\'s up',\n",
    "\t\t\t\t'brb' : 'be right back'\n",
    "\t\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer = WordReplacer(wordmapobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birthday\n"
     ]
    }
   ],
   "source": [
    "result = replacer.replace(\"bday\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's up\n"
     ]
    }
   ],
   "source": [
    "result = replacer.replace(\"sup\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be right back\n"
     ]
    }
   ],
   "source": [
    "result = replacer.replace(\"brb\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = word_tokenize('Sup! Awesome bday? brb!')\n",
    "sw2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sw:\t\n",
    "\tresult = replacer.replace(word)\n",
    "\tsw2 += result+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sup ! Awesome birthday ? be right back ! '"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace opposite meaning i.e Antonym\n",
    "from replacer import AntonymReplacer\n",
    "rep = AntonymReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courage\n",
      "light\n",
      "strong\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "antn = rep.replace('cowardice')\n",
    "print(antn)\n",
    "\n",
    "antn = rep.replace('heavy')\n",
    "print(antn)\n",
    "\n",
    "antn = rep.replace('weak')\n",
    "print(antn)\n",
    "\n",
    "antn = rep.replace('blue')\n",
    "print(antn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this man is fresh \n"
     ]
    }
   ],
   "source": [
    "sent = rep.negreplace('this man is not salty')\n",
    "print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
